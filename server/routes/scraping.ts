// server/routes/scraping.ts
import { Router } from 'express';
import { spawn, exec } from 'child_process';
import fs from 'fs/promises';
import path from 'path';
import { z } from 'zod';
import { storage } from '../storage';

const router = Router();

// Validation schemas
const triggerScrapingSchema = z.object({
  spiders: z.array(z.string()).optional(),
  maxItems: z.number().min(1).max(10000).optional(),
  concurrent: z.number().min(1).max(5).optional(),
  dryRun: z.boolean().optional(),
});

// In-memory store for scraping sessions
const scrapingSessions: Map<string, {
  id: string;
  status: 'running' | 'completed' | 'failed' | 'cancelled';
  startTime: Date;
  endTime?: Date;
  spiders: string[];
  progress: {
    spidersCompleted: number;
    totalSpiders: number;
    itemsScraped: number;
    errors: number;
  };
  results?: any;
  error?: string;
}> = new Map();

/**
 * GET /api/scraping/status
 * Get status of all scraping sessions
 */
router.get('/status', async (req, res) => {
  try {
    const sessions = Array.from(scrapingSessions.values()).map(session => ({
      id: session.id,\n      status: session.status,\n      startTime: session.startTime,\n      endTime: session.endTime,\n      progress: session.progress,\n      spiders: session.spiders\n    }));\n    \n    res.json({\n      success: true,\n      sessions,\n      activeSessions: sessions.filter(s => s.status === 'running').length\n    });\n  } catch (error) {\n    console.error('Error getting scraping status:', error);\n    res.status(500).json({ success: false, error: 'Failed to get scraping status' });\n  }\n});\n\n/**\n * GET /api/scraping/session/:sessionId\n * Get detailed status of a specific scraping session\n */\nrouter.get('/session/:sessionId', async (req, res) => {\n  try {\n    const { sessionId } = req.params;\n    const session = scrapingSessions.get(sessionId);\n    \n    if (!session) {\n      return res.status(404).json({ success: false, error: 'Session not found' });\n    }\n    \n    res.json({\n      success: true,\n      session\n    });\n  } catch (error) {\n    console.error('Error getting session details:', error);\n    res.status(500).json({ success: false, error: 'Failed to get session details' });\n  }\n});\n\n/**\n * POST /api/scraping/trigger\n * Trigger job scraping process\n */\nrouter.post('/trigger', async (req, res) => {\n  try {\n    // Validate request body\n    const validatedData = triggerScrapingSchema.parse(req.body);\n    \n    // Check if there's already a running session\n    const runningSessions = Array.from(scrapingSessions.values())\n      .filter(session => session.status === 'running');\n    \n    if (runningSessions.length >= 2) {\n      return res.status(429).json({\n        success: false,\n        error: 'Maximum number of concurrent scraping sessions reached'\n      });\n    }\n    \n    // Generate session ID\n    const sessionId = `scraping_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;\n    \n    // Create session record\n    const session = {\n      id: sessionId,\n      status: 'running' as const,\n      startTime: new Date(),\n      spiders: validatedData.spiders || ['gumtree_jobs'],\n      progress: {\n        spidersCompleted: 0,\n        totalSpiders: validatedData.spiders?.length || 1,\n        itemsScraped: 0,\n        errors: 0\n      }\n    };\n    \n    scrapingSessions.set(sessionId, session);\n    \n    // Start scraping process\n    startScrapingProcess(sessionId, validatedData);\n    \n    res.json({\n      success: true,\n      sessionId,\n      message: 'Scraping process started',\n      session: {\n        id: session.id,\n        status: session.status,\n        startTime: session.startTime,\n        spiders: session.spiders\n      }\n    });\n    \n  } catch (error) {\n    if (error instanceof z.ZodError) {\n      return res.status(400).json({\n        success: false,\n        error: 'Invalid request data',\n        details: error.errors\n      });\n    }\n    \n    console.error('Error triggering scraping:', error);\n    res.status(500).json({ success: false, error: 'Failed to trigger scraping' });\n  }\n});\n\n/**\n * POST /api/scraping/cancel/:sessionId\n * Cancel a running scraping session\n */\nrouter.post('/cancel/:sessionId', async (req, res) => {\n  try {\n    const { sessionId } = req.params;\n    const session = scrapingSessions.get(sessionId);\n    \n    if (!session) {\n      return res.status(404).json({ success: false, error: 'Session not found' });\n    }\n    \n    if (session.status !== 'running') {\n      return res.status(400).json({ \n        success: false, \n        error: 'Session is not running' \n      });\n    }\n    \n    // Update session status\n    session.status = 'cancelled';\n    session.endTime = new Date();\n    \n    // TODO: Kill the actual scraping process\n    // This would require tracking the process PID\n    \n    res.json({\n      success: true,\n      message: 'Scraping session cancelled'\n    });\n    \n  } catch (error) {\n    console.error('Error cancelling scraping session:', error);\n    res.status(500).json({ success: false, error: 'Failed to cancel session' });\n  }\n});\n\n/**\n * GET /api/scraping/logs/:sessionId\n * Get logs for a scraping session\n */\nrouter.get('/logs/:sessionId', async (req, res) => {\n  try {\n    const { sessionId } = req.params;\n    const logPath = path.join(__dirname, '../../scrapy_jobs/job_scraping.log');\n    \n    try {\n      const logContent = await fs.readFile(logPath, 'utf-8');\n      const sessionLogs = logContent\n        .split('\\n')\n        .filter(line => line.includes(sessionId) || line.includes('INFO'))\n        .slice(-100); // Last 100 relevant lines\n      \n      res.json({\n        success: true,\n        logs: sessionLogs\n      });\n    } catch (fileError) {\n      res.json({\n        success: true,\n        logs: ['Log file not found or not accessible'],\n        warning: 'Could not access log file'\n      });\n    }\n    \n  } catch (error) {\n    console.error('Error getting scraping logs:', error);\n    res.status(500).json({ success: false, error: 'Failed to get logs' });\n  }\n});\n\n/**\n * GET /api/scraping/stats\n * Get overall scraping statistics\n */\nrouter.get('/stats', async (req, res) => {\n  try {\n    // Get database statistics\n    const totalJobs = await storage.getJobs();\n    const recentJobs = totalJobs.filter(job => {\n      const createdAt = new Date(job.createdAt!);\n      const sevenDaysAgo = new Date();\n      sevenDaysAgo.setDate(sevenDaysAgo.getDate() - 7);\n      return createdAt > sevenDaysAgo;\n    });\n    \n    const companies = await storage.getCompanies();\n    \n    res.json({\n      success: true,\n      stats: {\n        totalJobs: totalJobs.length,\n        recentJobs: recentJobs.length,\n        totalCompanies: companies.length,\n        lastScrapingSession: Array.from(scrapingSessions.values())\n          .sort((a, b) => b.startTime.getTime() - a.startTime.getTime())[0]?.id || null,\n        scrapingHistory: {\n          totalSessions: scrapingSessions.size,\n          completedSessions: Array.from(scrapingSessions.values())\n            .filter(s => s.status === 'completed').length,\n          failedSessions: Array.from(scrapingSessions.values())\n            .filter(s => s.status === 'failed').length\n        }\n      }\n    });\n  } catch (error) {\n    console.error('Error getting scraping stats:', error);\n    res.status(500).json({ success: false, error: 'Failed to get statistics' });\n  }\n});\n\n/**\n * Internal function to start the scraping process\n */\nfunction startScrapingProcess(sessionId: string, options: any) {\n  const scrapyDir = path.join(__dirname, '../../scrapy_jobs');\n  const pythonScript = path.join(scrapyDir, 'run_scrapers.py');\n  \n  // Build command arguments\n  const args = ['python3', pythonScript];\n  \n  if (options.spiders && options.spiders.length === 1) {\n    args.push('--spider', options.spiders[0]);\n  }\n  \n  if (options.maxItems) {\n    args.push('--max-items', options.maxItems.toString());\n  }\n  \n  if (options.concurrent) {\n    args.push('--concurrent', options.concurrent.toString());\n  }\n  \n  if (options.dryRun) {\n    args.push('--dry-run');\n  }\n  \n  console.log(`Starting scraping process: ${args.join(' ')}`);\n  \n  // Start the process\n  const scrapingProcess = spawn(args[0], args.slice(1), {\n    cwd: scrapyDir,\n    stdio: ['ignore', 'pipe', 'pipe']\n  });\n  \n  const session = scrapingSessions.get(sessionId)!;\n  \n  // Handle process output\n  scrapingProcess.stdout.on('data', (data) => {\n    const output = data.toString();\n    console.log(`[${sessionId}] ${output}`);\n    \n    // Parse progress from output (implement based on your logging format)\n    // This is a simple example - you'd want more sophisticated parsing\n    if (output.includes('Spider') && output.includes('completed')) {\n      session.progress.spidersCompleted++;\n    }\n    if (output.includes('Scraped from')) {\n      session.progress.itemsScraped++;\n    }\n  });\n  \n  scrapingProcess.stderr.on('data', (data) => {\n    const error = data.toString();\n    console.error(`[${sessionId}] ERROR: ${error}`);\n    session.progress.errors++;\n  });\n  \n  scrapingProcess.on('close', (code) => {\n    session.endTime = new Date();\n    \n    if (code === 0) {\n      session.status = 'completed';\n      console.log(`[${sessionId}] Scraping completed successfully`);\n      \n      // Load and store results\n      loadScrapingResults(sessionId);\n    } else {\n      session.status = 'failed';\n      session.error = `Process exited with code ${code}`;\n      console.error(`[${sessionId}] Scraping failed with code ${code}`);\n    }\n  });\n  \n  scrapingProcess.on('error', (error) => {\n    session.status = 'failed';\n    session.endTime = new Date();\n    session.error = error.message;\n    console.error(`[${sessionId}] Process error:`, error);\n  });\n}\n\n/**\n * Load scraping results from the report file\n */\nasync function loadScrapingResults(sessionId: string) {\n  try {\n    const scrapyDir = path.join(__dirname, '../../scrapy_jobs');\n    \n    // Find the most recent scraping report\n    const files = await fs.readdir(scrapyDir);\n    const reportFiles = files\n      .filter(file => file.startsWith('scraping_report_') && file.endsWith('.json'))\n      .sort()\n      .reverse();\n    \n    if (reportFiles.length > 0) {\n      const reportPath = path.join(scrapyDir, reportFiles[0]);\n      const reportContent = await fs.readFile(reportPath, 'utf-8');\n      const results = JSON.parse(reportContent);\n      \n      const session = scrapingSessions.get(sessionId);\n      if (session) {\n        session.results = results;\n        session.progress.itemsScraped = results.statistics?.jobs_scraped || 0;\n      }\n    }\n  } catch (error) {\n    console.error(`Error loading results for session ${sessionId}:`, error);\n  }\n}\n\nexport default router;"
